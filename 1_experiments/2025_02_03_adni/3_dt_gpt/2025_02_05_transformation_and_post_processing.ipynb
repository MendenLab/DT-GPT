{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05.02.2025 - Transformation & Post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from pandas.tseries.offsets import MonthBegin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# From eval run: https://genentech.wandb.io/nikitamakarov/UC%20-%20ADNI/runs/tyhfwyg7?nw=nwusernikitamakarov\n",
    "generated_predictions_path = \"/pstore/data/dt-gpt/raw_experiments/uc2_nsclc/adni_dt_gpt/adni_dt_gpt/2025_02_03___17_32_50_444796/eval_meta_data/predictions_raw.csv\"\n",
    "generated_predictions = pd.read_csv(generated_predictions_path)\n",
    "\n",
    "# Get true predictions\n",
    "true_predictions = pd.read_csv(\"/home/makaron1/dt-gpt/uc4-alzheimers-disease/data/ADNI_short_DT_GPT/ADNI_short_test_ground_truth.csv\")\n",
    "\n",
    "# Target cols\n",
    "target_cols =  [\"CDRSB\", \"ADAS11\", \"MMSE\"]\n",
    "true_predictions = true_predictions[[\"PATIENT_ID\", \"MONTH\"] + target_cols]\n",
    "true_predictions = true_predictions.sort_values(by=[\"PATIENT_ID\", \"MONTH\"])\n",
    "\n",
    "# Mapping used throughout\n",
    "mapping = {\n",
    "    \"CDR-SB score\": \"CDRSB\",\n",
    "    \"ADAS11 score\": \"ADAS11\",\n",
    "    \"MMSE score\": \"MMSE\"\n",
    "}\n",
    "reverse_mapping = {v: k for k, v in mapping.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First do post-processing of responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#: sometimes the model repeats itself, so split anything after the first occurence of <patient_prediction> and keep only first part\n",
    "processed_predictions = generated_predictions.copy()\n",
    "processed_predictions[\"responses\"] = processed_predictions[\"responses\"].apply(lambda x: x.split(\"<patient_prediction>\")[0])\n",
    "\n",
    "#: apply stripping of whitespaces\n",
    "processed_predictions[\"responses\"] = processed_predictions[\"responses\"].apply(lambda x: x.strip())\n",
    "\n",
    "#: sometimes model makes 2 \"]}\" at the end instead of one -> in those cases keep only 1\n",
    "processed_predictions[\"responses\"] = processed_predictions[\"responses\"].apply(lambda x: x.split(\"]}\")[0] + \"]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_predictions.iloc[10,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Length of processed predictions: \", len(processed_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process into dataframes, then average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_dfs = []\n",
    "\n",
    "# Iterate through each row in the processed_predictions dataframe\n",
    "for idx, row in tqdm(processed_predictions.iterrows()):\n",
    "    patient_id = row[\"patientid\"]\n",
    "    \n",
    "    try:\n",
    "        # Parse the JSON string into a dictionary\n",
    "        responses = json.loads(row[\"responses\"])\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Invalid JSON for patient {patient_id} at index {idx}. Skipping.\")\n",
    "        continue  # Skip to the next iteration if JSON is invalid\n",
    "\n",
    "    # Extract the true predictions for the current patient\n",
    "    true_df = true_predictions[true_predictions[\"PATIENT_ID\"] == patient_id]\n",
    "\n",
    "    # Initialize a dataframe for the generated data with PATIENT_ID and MONTH\n",
    "    generated_patient_df = pd.DataFrame({\n",
    "        \"PATIENT_ID\": patient_id,\n",
    "        \"MONTH\": true_df[\"MONTH\"]\n",
    "    })\n",
    "\n",
    "    # Iterate through each target column to align generated values\n",
    "    for target_col in target_cols:\n",
    "        # Retrieve the generated values for the current target column\n",
    "        generated_values = responses.get(reverse_mapping[target_col], [])\n",
    "\n",
    "        # Identify the months where the true data for this target is not missing\n",
    "        non_na_true = true_df[[\"PATIENT_ID\", \"MONTH\", target_col]].dropna(subset=[target_col]).sort_values(by=\"MONTH\").reset_index(drop=True)\n",
    "\n",
    "        # Check if the number of generated values matches the number of non-missing entries\n",
    "        if len(generated_values) != len(non_na_true):\n",
    "            # If generated too much, cut the excess\n",
    "            if len(generated_values) > len(non_na_true):\n",
    "                print(f\"Generated too many values for '{target_col}' in patient '{patient_id}': \"\n",
    "                      f\"expected {len(non_na_true)}, got {len(generated_values)}. Cutting the excess.\")\n",
    "                generated_values = generated_values[:len(non_na_true)]\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Length mismatch for '{target_col}' in patient '{patient_id}': \"\n",
    "                    f\"expected {len(non_na_true)}, got {len(generated_values)}.\"\n",
    "                )\n",
    "\n",
    "        # Initialize the target column with NaNs\n",
    "        generated_patient_df[target_col] = np.nan\n",
    "\n",
    "        # Assign the generated values to the corresponding months\n",
    "        generated_patient_df.loc[\n",
    "            generated_patient_df[\"MONTH\"].isin(non_na_true[\"MONTH\"]),\n",
    "            target_col\n",
    "        ] = generated_values\n",
    "\n",
    "    # Append the generated dataframe for the current patient to the list\n",
    "    generated_dfs.append(generated_patient_df)\n",
    "\n",
    "# Concatenate all generated dataframes into a single dataframe\n",
    "generated_df = pd.concat(generated_dfs, ignore_index=True)\n",
    "\n",
    "# (Optional) If you want to verify the alignment, you can perform additional checks here\n",
    "# For example:\n",
    "# assert generated_df.isna().sum().sum() == 0, \"There are still missing values in the generated dataframe.\"\n",
    "\n",
    "# Display the first few rows of the generated dataframe\n",
    "print(generated_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now average by patient and month\n",
    "generated_df_averaged = generated_df.groupby([\"PATIENT_ID\", \"MONTH\"]).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check that average is correct for first patient\n",
    "patient_id_0 = generated_df_averaged[\"PATIENT_ID\"].iloc[0]\n",
    "original_df = generated_df[generated_df[\"PATIENT_ID\"] == patient_id_0]\n",
    "original_df_first_time = original_df[original_df[\"MONTH\"] == original_df[\"MONTH\"].min()]\n",
    "\n",
    "assert generated_df_averaged[generated_df_averaged[\"PATIENT_ID\"] == patient_id_0][\"CDRSB\"].iloc[0] == original_df_first_time[\"CDRSB\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save DT-GPT Outputs in common format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_gpt_outputs = generated_df_averaged_no_na.copy()\n",
    "\n",
    "# Standardize\n",
    "dt_gpt_outputs[\"CDRSB\"] = (dt_gpt_outputs[\"CDRSB\"] - mean_CDSSB) / std_CDRSB\n",
    "dt_gpt_outputs[\"ADAS11\"] = (dt_gpt_outputs[\"ADAS11\"] - mean_ADAS11) / std_ADAS11\n",
    "dt_gpt_outputs[\"MMSE\"] = (dt_gpt_outputs[\"MMSE\"] - mean_MMSE) / std_MMSE\n",
    "\n",
    "# Add fake patient_sample_index\n",
    "dt_gpt_outputs[\"patient_sample_index\"] = \"split_0\"\n",
    "\n",
    "dt_gpt_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "dt_gpt_outputs.to_csv(\"/home/makaron1/dt-gpt/uc2_nsclc/2_experiments/2025_02_03_adni/3_dt_gpt/outputs/dt_gpt_outputs.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uc1_gpu_venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
